{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Save summary and push to GitHub\nimport json\n\nsummary_04 = {\n    'model': config.model_name,\n    'adapter_params': sum(p.numel() for p in adapter.parameters()),\n    'final_loss': float(loss_hist[-1]),\n    'agreement_weights': reweighter.w.data.tolist(),\n    'num_epochs': len(loss_hist),\n}\n\nos.makedirs('results', exist_ok=True)\nwith open(f'results/04_adapter_summary_{MODEL_TAG}.json', 'w') as f:\n    json.dump(summary_04, f, indent=2)\n\n!cp adapter_training_loss.png results/adapter_training_loss_{MODEL_TAG}.png 2>/dev/null || true\n\n!git pull --rebase 2>/dev/null || true\n!git add results/\n!git commit -m \"Add Notebook 04 results: adapter training ({MODEL_TAG})\"\n!git push\n\nprint('Results pushed to GitHub!')\nprint('(Adapter weights are on Google Drive)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Train Prefix Adapter\n",
    "\n",
    "Loads the LLM, trained autoencoder, and hidden states, then trains\n",
    "the prefix adapter and agreement weights.\n",
    "\n",
    "**Estimated time: ~1-2 hours on T4/A100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport os\ntry:\n    from google.colab import userdata\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n    REPO_URL = f'https://{GITHUB_TOKEN}@github.com/AUMEZAK/thoughtcomm.git'\nexcept Exception:\n    GITHUB_TOKEN = None\n    REPO_URL = 'https://github.com/AUMEZAK/thoughtcomm.git'\n\n!git clone {REPO_URL} thoughtcomm 2>/dev/null || echo 'Already cloned'\n%cd thoughtcomm\n!pip install -e . -q\n\n!git config user.email \"colab@thoughtcomm.dev\"\n!git config user.name \"ThoughtComm Colab\"\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/thoughtcomm_checkpoints/'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from configs.config import ThoughtCommConfig\n",
    "from models.model_utils import load_model_and_tokenizer\n",
    "from models.autoencoder import SparsityRegularizedAE\n",
    "from models.prefix_adapter import PrefixAdapter\n",
    "from pipeline.agreement import AgreementReweighter\n",
    "from training.train_adapter import train_adapter\n",
    "from utils.memory import print_memory_stats\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = ThoughtCommConfig.for_qwen_0_6b(device=device)\n",
    "# config = ThoughtCommConfig.for_phi4_mini(device=device)\n",
    "MODEL_TAG = config.model_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM\n",
    "model, tokenizer = load_model_and_tokenizer(config.model_name, dtype=config.dtype)\n",
    "print_memory_stats('After model load: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained AE and B matrix\n",
    "ae_dir = os.path.join(SAVE_DIR, f'{MODEL_TAG}_ae')\n",
    "ae_model = SparsityRegularizedAE(\n",
    "    n_h=config.n_h, n_z=config.n_z,\n",
    "    hidden_dim=config.ae_hidden, num_layers=config.ae_num_layers\n",
    ")\n",
    "ae_model.load_state_dict(torch.load(os.path.join(ae_dir, 'ae_model.pt'), map_location='cpu'))\n",
    "ae_model = ae_model.to(device)\n",
    "\n",
    "B = torch.load(os.path.join(ae_dir, 'B_matrix.pt'), map_location='cpu')\n",
    "print(f'AE loaded. B shape: {B.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hidden states\n",
    "math_data = torch.load(\n",
    "    os.path.join(SAVE_DIR, f'{MODEL_TAG}_math', 'hidden_states.pt'),\n",
    "    map_location='cpu'\n",
    ")\n",
    "H_train = math_data['H']\n",
    "metadata = math_data['metadata']\n",
    "print(f'H_train: {H_train.shape}, metadata: {len(metadata)} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adapter and reweighter\n",
    "reweighter = AgreementReweighter(B, config)\n",
    "adapter = PrefixAdapter(\n",
    "    n_z=config.n_z,\n",
    "    hidden_size=config.hidden_size,\n",
    "    prefix_length=config.prefix_length,\n",
    "    adapter_hidden=config.adapter_hidden,\n",
    ")\n",
    "\n",
    "print(f'Adapter params: {sum(p.numel() for p in adapter.parameters()):,}')\n",
    "print(f'Agreement weights: {reweighter.w}')\n",
    "print(reweighter.get_agreement_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train adapter\n",
    "adapter, reweighter, loss_hist = train_adapter(\n",
    "    model, tokenizer, ae_model, reweighter, adapter,\n",
    "    H_train, metadata, config, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot adapter training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Adapter Training Loss (L_comm)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('adapter_training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Learned agreement weights: {reweighter.w.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter and reweighter\n",
    "adapter_dir = os.path.join(SAVE_DIR, f'{MODEL_TAG}_adapter')\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "\n",
    "torch.save(adapter.state_dict(), os.path.join(adapter_dir, 'adapter.pt'))\n",
    "torch.save(reweighter.state_dict(), os.path.join(adapter_dir, 'reweighter.pt'))\n",
    "torch.save(loss_hist, os.path.join(adapter_dir, 'adapter_loss.pt'))\n",
    "\n",
    "print(f'Saved to {adapter_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Push Results to GitHub",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}