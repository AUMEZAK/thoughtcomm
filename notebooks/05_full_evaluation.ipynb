{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Save final results as JSON and push to GitHub\nimport json\n\nfinal_results = {\n    'model': config.model_name,\n    'single_answer': {\n        'math_acc': float(sa_math_acc),\n        'gsm8k_acc': float(sa_gsm8k_acc),\n    },\n    'debate_only': {\n        'math_acc_per_agent': [float(a) for a in debate_math_accs],\n        'math_acc_avg': float(np.mean(debate_math_accs)),\n        'math_consensus': float(debate_math_consensus),\n    },\n    'thoughtcomm': {\n        'math_acc_per_agent': [float(a) for a in tc_math_accs],\n        'math_acc_avg': float(np.mean(tc_math_accs)),\n        'math_consensus': float(tc_math_consensus),\n        'gsm8k_acc_per_agent': [float(a) for a in tc_gsm8k_accs],\n        'gsm8k_acc_avg': float(np.mean(tc_gsm8k_accs)),\n        'gsm8k_consensus': float(tc_gsm8k_consensus),\n    },\n}\n\nos.makedirs('results', exist_ok=True)\nwith open(f'results/05_final_results_{MODEL_TAG}.json', 'w') as f:\n    json.dump(final_results, f, indent=2)\n\n!git pull --rebase 2>/dev/null || true\n!git add results/\n!git commit -m \"Add Notebook 05 results: full evaluation Table 1 ({MODEL_TAG})\"\n!git push\n\nprint('Final results pushed to GitHub!')\nprint(f'View at: https://github.com/AUMEZAK/thoughtcomm/tree/main/results')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Full Evaluation — Reproduce Table 1\n",
    "\n",
    "Loads all trained components and evaluates on MATH and GSM8K.\n",
    "Compares:\n",
    "1. Single Answer baseline\n",
    "2. Debate-only baseline\n",
    "3. ThoughtComm (ours)\n",
    "\n",
    "Reproduces Table 1, Fig 5 (prefix length ablation), Fig 6 (round scaling).\n",
    "\n",
    "**Estimated time: ~2-4 hours per model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport os\ntry:\n    from google.colab import userdata\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n    REPO_URL = f'https://{GITHUB_TOKEN}@github.com/AUMEZAK/thoughtcomm.git'\nexcept Exception:\n    GITHUB_TOKEN = None\n    REPO_URL = 'https://github.com/AUMEZAK/thoughtcomm.git'\n\n!git clone {REPO_URL} thoughtcomm 2>/dev/null || echo 'Already cloned'\n%cd thoughtcomm\n!pip install -e . -q\n\n!git config user.email \"colab@thoughtcomm.dev\"\n!git config user.name \"ThoughtComm Colab\"\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/thoughtcomm_checkpoints/'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from configs.config import ThoughtCommConfig\n",
    "from models.model_utils import load_model_and_tokenizer\n",
    "from models.autoencoder import SparsityRegularizedAE\n",
    "from models.prefix_adapter import PrefixAdapter\n",
    "from pipeline.agreement import AgreementReweighter\n",
    "from pipeline.thought_comm import ThoughtCommPipeline, run_single_answer_baseline, run_debate_baseline\n",
    "from data.math_data import load_math_dataset\n",
    "from data.gsm8k_data import load_gsm8k_dataset\n",
    "from evaluation.metrics import compute_accuracy, compute_consensus\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = ThoughtCommConfig.for_qwen_0_6b(device=device)\n",
    "# config = ThoughtCommConfig.for_phi4_mini(device=device)\n",
    "MODEL_TAG = config.model_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model, tokenizer = load_model_and_tokenizer(config.model_name, dtype=config.dtype)\n",
    "\n",
    "# Load trained components\n",
    "ae_dir = os.path.join(SAVE_DIR, f'{MODEL_TAG}_ae')\n",
    "adapter_dir = os.path.join(SAVE_DIR, f'{MODEL_TAG}_adapter')\n",
    "\n",
    "ae_model = SparsityRegularizedAE(\n",
    "    n_h=config.n_h, n_z=config.n_z,\n",
    "    hidden_dim=config.ae_hidden, num_layers=config.ae_num_layers\n",
    ").to(device)\n",
    "ae_model.load_state_dict(torch.load(os.path.join(ae_dir, 'ae_model.pt'), map_location=device))\n",
    "\n",
    "B = torch.load(os.path.join(ae_dir, 'B_matrix.pt'), map_location='cpu')\n",
    "reweighter = AgreementReweighter(B, config).to(device)\n",
    "reweighter.load_state_dict(torch.load(os.path.join(adapter_dir, 'reweighter.pt'), map_location=device))\n",
    "\n",
    "adapter = PrefixAdapter(\n",
    "    n_z=config.n_z, hidden_size=config.hidden_size,\n",
    "    prefix_length=config.prefix_length, adapter_hidden=config.adapter_hidden\n",
    ").to(device)\n",
    "adapter.load_state_dict(torch.load(os.path.join(adapter_dir, 'adapter.pt'), map_location=device))\n",
    "\n",
    "print('All components loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load eval datasets\n",
    "_, math_eval = load_math_dataset(num_train=500, num_eval=500, level=3)\n",
    "_, gsm8k_eval = load_gsm8k_dataset(num_train=500, num_eval=500)\n",
    "print(f'MATH eval: {len(math_eval)}, GSM8K eval: {len(gsm8k_eval)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Answer Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATH - Single Answer\n",
    "sa_math = run_single_answer_baseline(model, tokenizer, math_eval, config, 'math')\n",
    "sa_math_acc, _ = compute_accuracy(sa_math, [e['answer'] for e in math_eval], 'math')\n",
    "print(f'Single Answer MATH: {sa_math_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSM8K - Single Answer\n",
    "sa_gsm8k = run_single_answer_baseline(model, tokenizer, gsm8k_eval, config, 'gsm8k')\n",
    "sa_gsm8k_acc, _ = compute_accuracy(sa_gsm8k, [e['answer'] for e in gsm8k_eval], 'gsm8k')\n",
    "print(f'Single Answer GSM8K: {sa_gsm8k_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Debate-Only Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATH - Debate only\n",
    "debate_math = run_debate_baseline(model, tokenizer, math_eval, config)\n",
    "\n",
    "# Accuracy: use majority vote from final round agents\n",
    "debate_math_accs = []\n",
    "for agent_idx in range(config.num_agents):\n",
    "    agent_resps = [fr[agent_idx] for fr in debate_math['final_responses']]\n",
    "    acc, _ = compute_accuracy(agent_resps, [e['answer'] for e in math_eval], 'math')\n",
    "    debate_math_accs.append(acc)\n",
    "    print(f'  Agent {agent_idx} MATH acc: {acc:.2f}%')\n",
    "\n",
    "debate_math_consensus = compute_consensus(debate_math['final_responses'], 'math')\n",
    "print(f'Debate MATH avg acc: {np.mean(debate_math_accs):.2f}%, consensus: {debate_math_consensus:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ThoughtComm (Ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ThoughtComm pipeline\n",
    "tc = ThoughtCommPipeline(model, tokenizer, ae_model, reweighter, adapter, config)\n",
    "\n",
    "# MATH evaluation\n",
    "tc_math = tc.evaluate(math_eval, 'math')\n",
    "\n",
    "tc_math_accs = []\n",
    "for agent_idx in range(config.num_agents):\n",
    "    agent_resps = [fr[agent_idx] for fr in tc_math['final_responses']]\n",
    "    acc, _ = compute_accuracy(agent_resps, tc_math['ground_truths'], 'math')\n",
    "    tc_math_accs.append(acc)\n",
    "    print(f'  Agent {agent_idx} MATH acc: {acc:.2f}%')\n",
    "\n",
    "tc_math_consensus = compute_consensus(tc_math['final_responses'], 'math')\n",
    "print(f'ThoughtComm MATH avg acc: {np.mean(tc_math_accs):.2f}%, consensus: {tc_math_consensus:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSM8K evaluation\n",
    "tc_gsm8k = tc.evaluate(gsm8k_eval, 'gsm8k')\n",
    "\n",
    "tc_gsm8k_accs = []\n",
    "for agent_idx in range(config.num_agents):\n",
    "    agent_resps = [fr[agent_idx] for fr in tc_gsm8k['final_responses']]\n",
    "    acc, _ = compute_accuracy(agent_resps, tc_gsm8k['ground_truths'], 'gsm8k')\n",
    "    tc_gsm8k_accs.append(acc)\n",
    "\n",
    "tc_gsm8k_consensus = compute_consensus(tc_gsm8k['final_responses'], 'gsm8k')\n",
    "print(f'ThoughtComm GSM8K avg acc: {np.mean(tc_gsm8k_accs):.2f}%, consensus: {tc_gsm8k_consensus:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Summary (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print(f'Table 1 Results — {config.model_name}')\n",
    "print('=' * 70)\n",
    "print(f'{\"Method\":<25} {\"MATH Acc (%)\":<15} {\"MATH Cons (%)\":<15} {\"GSM8K Acc (%)\":<15} {\"GSM8K Cons (%)\":<15}')\n",
    "print('-' * 70)\n",
    "print(f'{\"Single Answer\":<25} {sa_math_acc:<15.2f} {\"N/A\":<15} {sa_gsm8k_acc:<15.2f} {\"N/A\":<15}')\n",
    "print(f'{\"Debate Only\":<25} {np.mean(debate_math_accs):<15.2f} {debate_math_consensus:<15.2f} {\"--\":<15} {\"--\":<15}')\n",
    "print(f'{\"ThoughtComm (Ours)\":<25} {np.mean(tc_math_accs):<15.2f} {tc_math_consensus:<15.2f} {np.mean(tc_gsm8k_accs):<15.2f} {tc_gsm8k_consensus:<15.2f}')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    'model': config.model_name,\n",
    "    'single_answer_math': sa_math_acc,\n",
    "    'single_answer_gsm8k': sa_gsm8k_acc,\n",
    "    'debate_math_acc': np.mean(debate_math_accs),\n",
    "    'debate_math_consensus': debate_math_consensus,\n",
    "    'thoughtcomm_math_acc': np.mean(tc_math_accs),\n",
    "    'thoughtcomm_math_consensus': tc_math_consensus,\n",
    "    'thoughtcomm_gsm8k_acc': np.mean(tc_gsm8k_accs),\n",
    "    'thoughtcomm_gsm8k_consensus': tc_gsm8k_consensus,\n",
    "}\n",
    "\n",
    "results_path = os.path.join(SAVE_DIR, f'{MODEL_TAG}_results.pt')\n",
    "torch.save(results, results_path)\n",
    "print(f'Results saved to {results_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Push Results to GitHub",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}