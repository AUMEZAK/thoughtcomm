{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Train Sparsity-Regularized Autoencoder\n",
    "\n",
    "Loads hidden states from Google Drive (collected in Notebook 2),\n",
    "trains the sparsity-regularized autoencoder, and extracts the\n",
    "binary Jacobian pattern B(J_f).\n",
    "\n",
    "**No LLM needed. Estimated time: ~30 minutes on T4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport os\ntry:\n    from google.colab import userdata\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n    REPO_URL = f'https://{GITHUB_TOKEN}@github.com/AUMEZAK/thoughtcomm.git'\nexcept Exception:\n    GITHUB_TOKEN = None\n    REPO_URL = 'https://github.com/AUMEZAK/thoughtcomm.git'\n\n!git clone {REPO_URL} thoughtcomm 2>/dev/null || echo 'Already cloned'\n%cd thoughtcomm\n!pip install -e . -q\n\n!git config user.email \"colab@thoughtcomm.dev\"\n!git config user.name \"ThoughtComm Colab\"\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/thoughtcomm_checkpoints/'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from configs.config import ThoughtCommConfig\n",
    "from training.train_autoencoder import train_autoencoder\n",
    "from training.jacobian_utils import compute_binary_pattern\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Choose model config (must match Notebook 2)\n",
    "config = ThoughtCommConfig.for_qwen_0_6b(device=device)\n",
    "# config = ThoughtCommConfig.for_phi4_mini(device=device)\n",
    "\n",
    "MODEL_TAG = config.model_name.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hidden states from Drive\n",
    "math_path = os.path.join(SAVE_DIR, f'{MODEL_TAG}_math', 'hidden_states.pt')\n",
    "data = torch.load(math_path, map_location='cpu')\n",
    "H_train = data['H']\n",
    "metadata = data['metadata']\n",
    "\n",
    "print(f'H_train shape: {H_train.shape}')\n",
    "print(f'Metadata entries: {len(metadata)}')\n",
    "print(f'Expected n_h: {config.n_h}')\n",
    "assert H_train.shape[1] == config.n_h, f'Shape mismatch: {H_train.shape[1]} != {config.n_h}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AE\n",
    "ae_model, loss_history = train_autoencoder(H_train, config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(loss_history['rec'])\n",
    "axes[0].set_title('Reconstruction Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(loss_history['jac'])\n",
    "axes[1].set_title('Jacobian L1')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "\n",
    "axes[2].plot(loss_history['total'])\n",
    "axes[2].set_title('Total Loss')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ae_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute B matrix\n",
    "print('Computing full Jacobian and B matrix...')\n",
    "with torch.no_grad():\n",
    "    Z_sample = ae_model.encode(H_train[:64].float().to(device))\n",
    "\n",
    "B = compute_binary_pattern(\n",
    "    ae_model.decoder, Z_sample,\n",
    "    threshold=config.jacobian_threshold,\n",
    "    sub_batch=8, device=device\n",
    ")\n",
    "print(f'B shape: {B.shape}')\n",
    "print(f'B sparsity: {1 - B.float().mean():.3f}')\n",
    "print(f'B non-zero entries: {B.sum().item()} / {B.numel()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize B matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(B.numpy(), cmap='Blues', aspect='auto')\n",
    "plt.colorbar(label='Dependency')\n",
    "\n",
    "# Mark agent boundaries\n",
    "for k in range(1, config.num_agents):\n",
    "    plt.axhline(y=k * config.hidden_size - 0.5, color='red', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.xlabel('Latent thought dimensions', fontsize=12)\n",
    "plt.ylabel('Agent hidden state dimensions', fontsize=12)\n",
    "plt.title('Binary Jacobian Pattern B(J_f)', fontsize=14)\n",
    "\n",
    "# Add agent labels\n",
    "for k in range(config.num_agents):\n",
    "    y_pos = k * config.hidden_size + config.hidden_size // 2\n",
    "    plt.text(-30, y_pos, f'Agent {k+1}', fontsize=11, va='center', fontweight='bold')\n",
    "\n",
    "plt.savefig('b_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze agreement structure\n",
    "from pipeline.agreement import AgreementReweighter\n",
    "\n",
    "reweighter = AgreementReweighter(B, config)\n",
    "stats = reweighter.get_agreement_stats()\n",
    "print('Agreement structure:')\n",
    "for k, v in stats['agreement_distribution'].items():\n",
    "    print(f'  {k}: {v} dimensions')\n",
    "print(f'Per-agent relevant dims: {stats[\"per_agent_relevant\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained AE and B matrix\n",
    "ae_save_dir = os.path.join(SAVE_DIR, f'{MODEL_TAG}_ae')\n",
    "os.makedirs(ae_save_dir, exist_ok=True)\n",
    "\n",
    "torch.save(ae_model.state_dict(), os.path.join(ae_save_dir, 'ae_model.pt'))\n",
    "torch.save(B, os.path.join(ae_save_dir, 'B_matrix.pt'))\n",
    "torch.save(loss_history, os.path.join(ae_save_dir, 'loss_history.pt'))\n",
    "\n",
    "print(f'Saved to {ae_save_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Push Results to GitHub",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save summary and figures, push to GitHub\nimport json\n\nsummary_03 = {\n    'model': config.model_name,\n    'ae_final_rec_loss': float(loss_history['rec'][-1]),\n    'ae_final_jac_loss': float(loss_history['jac'][-1]),\n    'ae_final_total_loss': float(loss_history['total'][-1]),\n    'b_shape': list(B.shape),\n    'b_sparsity': float(1 - B.float().mean()),\n    'b_nonzero': int(B.sum().item()),\n    'b_total': int(B.numel()),\n    'agreement_stats': stats,\n}\n\nos.makedirs('results', exist_ok=True)\nwith open(f'results/03_autoencoder_summary_{MODEL_TAG}.json', 'w') as f:\n    json.dump(summary_03, f, indent=2)\n\n# Copy figures\n!cp ae_training_curves.png results/ae_training_curves_{MODEL_TAG}.png 2>/dev/null || true\n!cp b_matrix.png results/b_matrix_{MODEL_TAG}.png 2>/dev/null || true\n\n!git pull --rebase 2>/dev/null || true\n!git add results/\n!git commit -m \"Add Notebook 03 results: AE training + B matrix ({MODEL_TAG})\"\n!git push\n\nprint('Results pushed to GitHub!')\nprint('(AE weights and B matrix are on Google Drive)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}